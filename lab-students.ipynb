{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T09:25:06.462154Z",
     "start_time": "2023-04-19T09:25:05.043600Z"
    },
    "tags": []
   },
   "source": [
    "# Gaussian Processes and Hyperparameter optimization with Bayesian optimization\n",
    "\n",
    "In this lab, we aim to get a better understanding of Gaussian Processes (GPs) and Bayesian Optimzation (BO). \n",
    "\n",
    "GPs are regression models with well calibrated uncertainty estimates and Bayesian optimization (BO) is a tool to optimize black-box function, i.e., functions with unknown structure that are expensive to evaluate.\n",
    "BO consists of the following steps that are repeated until one runs out of time or money:\n",
    "\n",
    "1. Learn a model of the function\n",
    "2. Using this model, find a promising point to evaluate next\n",
    "3. Evaluate the point\n",
    "4. Refine the model using the new observation\n",
    "\n",
    "The lab consists of two parts. In **PART 1**, we will write our own implementation of a Gaussian Process to understand its differents parts. We will also use it for some simple BO. In **PART 2**, we will try to apply state-of-the-art BO tools to optimize the hyperparameters of a Artifical Neural Network. We will use gpytorch for GPs and botorch for BO.\n",
    "\n",
    "***\n",
    "## PART 1\n",
    "***\n",
    "In this part we will implement our own GP and test it in a simple BO setting.\n",
    "\n",
    "\n",
    "_Tips_:\n",
    "This lab will require you to work with a lot of matrices both in numpy and in torch. You can use `@` to do matrix multiplication. Also, when working with matrices, keeping track of matrix dimensions is essential. Make sure to keep an eye on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:02.567081244Z",
     "start_time": "2023-05-08T11:54:02.564786687Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Callable                  # typing is a package that provides special type-hints to \n",
    "                                             # function defintions. 'Callable' is a type hint for a function.\n",
    "                                             # That is, if a function func(c: Callable):... takes a Callable c\n",
    "                                             # as input, then c should be a function itself.\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp                           # Scipy is an optimization library\n",
    "import seaborn as sns                        # Seaborn is a package for making nicer plots\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We first define a synthetic benchmark function we will work on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:02.793540907Z",
     "start_time": "2023-05-08T11:54:02.567525961Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fn(\n",
    "        x: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    y = np.sin(x) + np.sin((10.0 / 4.0) * x)\n",
    "    return y\n",
    "\n",
    "\n",
    "# define lower and upper bounds of the function: we will only consider the function within these bounds\n",
    "lb, ub = -2.7, 7.5\n",
    "\n",
    "plt.plot(np.linspace(lb, ub, 200), fn(np.linspace(lb, ub, 200)), label='f(x)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Defining the training and test data\n",
    "Our goal is to eventually train a GP regressor, and for this we will need to have a training data set. The function in the plot above is the true function we are trying to predict, and the red dots in the plot below are the data points that we will train our model on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:02.849967898Z",
     "start_time": "2023-05-08T11:54:02.795115052Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train = np.random.RandomState(1).rand(5) * (ub - lb) + lb\n",
    "x_train = x_train[:, np.newaxis]\n",
    "y_train = fn(x_train)\n",
    "\n",
    "x_test = np.linspace(lb, ub, 250)[:, np.newaxis]\n",
    "y_test = fn(x_test)\n",
    "\n",
    "plt.plot(np.linspace(lb, ub, 200), fn(np.linspace(lb, ub, 200)), label='f(x)')\n",
    "plt.scatter(x_train, y_train, color=\"red\", zorder=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Gaussian process\n",
    "\n",
    "One view of GPs is as the assumption that any set of y-values ($y_1, y_2,..,y_n$) follows a joint normal distribution with some mean $\\mu$ and covariance matrix $K$, where the elements of $K$ are defined by the kernel function $K_{ij} = k(x_i, x_j)$. \n",
    "\n",
    "The mean function describes the average value of a point and the covariance function essentially captures how much two points are correlated.\n",
    "\n",
    "In our example, we will $\\mu=0$ use the RBF kernel:\n",
    "\n",
    "$$\n",
    " k(\\mathbf{x},\\mathbf{x'}) = \\exp\\left(-\\frac{||\\mathbf{x}-\\mathbf{x}'||^2}{2l^2}\\right).\n",
    "$$\n",
    "\n",
    "This kernel-function grows larger the closer $\\mathbf{x}$ and $\\mathbf{x'}$ are to each other, i.e., it assumes that the relation of two points is only determined by their distance. Such kernels are called stationary. There are other kernels whose value depends on the definite location of the points, though we will not see them in this lab. The distance is scaled by the parameter $l$ and we will see later how it affects the GP as well as how it can be learned from the data.\n",
    "\n",
    "Under the assumption that \n",
    "\n",
    "$$\n",
    "(y_1, y_2, ..,y_n) \\sim \\mathcal{GP}(\\mu, K),\n",
    "$$\n",
    "\n",
    "we can then make predictions for new points through the standard rules for conditioning for multivariate Gaussian distributions. If we have training data $(y_1, y_2, ..,y_n)$ and want to predict a new point $y^*$ for features $x^*$, then with\n",
    "\n",
    "$$\n",
    "K = \\begin{bmatrix}\n",
    "K & k^* \\\\\n",
    "k^* & k^{**}\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "$$\n",
    "k^* = (k(x_1, x^*), k(x_2, x^*), .., k(x_n, x^*),\n",
    "$$\n",
    "$$\n",
    "\\text{and}~~ k^{**} = k(x^*, x^*)\n",
    "$$\n",
    "we have that \n",
    "$$\n",
    "y^*\\sim \\mathcal{N}(k^*K^{-1}y, k^{**} - k^*K^{-1}k^{*T})\n",
    "$$\n",
    "\n",
    "---\n",
    "**Bonus info:**\n",
    "\n",
    "A GP can be seen as a probability distribution over _functions_. Hence, we can sample functions f from a GP and we write that they are distributed according to:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) \\sim \\mathcal{GP}(\\mu(\\mathbf{x}), k(\\mathbf{x},\\mathbf{x'})).\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining an RBF kernel with a global lengthscale\n",
    "\n",
    "The kernel function decides how \"connected\" two data points $x_1$ and $x_2$ are.\n",
    "\n",
    "**Your task:** Implement the RBF kernel as described above.\n",
    "\n",
    "**Question:** What does a return value of 1 mean, what does a return value of 0 mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:02.850280600Z",
     "start_time": "2023-05-08T11:54:02.838935547Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def kern(X: np.ndarray, X_prime: np.ndarray, ls: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns the covariance matrix between X and X'. \n",
    "    \n",
    "    NOTE: This function should handle calculating the kernel function between several data points at once.\n",
    "    Both X and X' can be considered matrices with dimensions (nxd) and (mxd), respectively. The output\n",
    "    should hence be a matrix of shape (nxm) and contain the value of the kernel function between every pair between X and X'. \n",
    "    \n",
    "    If X or X' are 1d vectors (with d elements), we first reshape them to (1,d)-matrices.\n",
    "    \"\"\"\n",
    "\n",
    "    if X.ndim == 1:\n",
    "        X = X.reshape(1, -1)\n",
    "    if X_prime.ndim == 1:\n",
    "        X_prime = X_prime.reshape(1, -1)\n",
    "        \n",
    "    # ‚û°Ô∏è TODO : implement the RBF kernel ‚¨ÖÔ∏è\n",
    "    \n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Safety check\n",
    "\n",
    "The cell below should display the following image:\n",
    "\n",
    "![RBF kernel plot](kernel_plot.png)\n",
    "\n",
    "**Question:** What does this image show?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:03.312165140Z",
     "start_time": "2023-05-08T11:54:02.839126389Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "GRANULARITY = 50\n",
    "\n",
    "x = np.linspace(-2, 2, GRANULARITY).reshape(-1, 1)\n",
    "y = np.linspace(-2, 2, GRANULARITY).reshape(-1, 1)\n",
    "\n",
    "xlabels = [''] * GRANULARITY\n",
    "ylabels = [''] * GRANULARITY\n",
    "\n",
    "for idx in np.arange(GRANULARITY)[::5]:\n",
    "    xlabels[idx] = f\"{x.squeeze()[idx]:.2f}\"\n",
    "    ylabels[idx] = f\"{y.squeeze()[idx]:.2f}\"\n",
    "\n",
    "z = kern(x, y, 0.5)\n",
    "assert z.ndim == 2, \"kern must be a two dimensional matrix\"\n",
    "assert z.shape == (x.shape[0], x.shape[0]), f\"kern should in this case yield a {x.shape[0]} by {x.shape[0]} matrix\"\n",
    "sns.heatmap(z, xticklabels=xlabels, yticklabels=ylabels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a mean function (constant zero)\n",
    "\n",
    "Surprisingly, the mean function is not really important. \n",
    "All we are interested in is the posterior of the GP, i.e., the distribution after we have seen some data.\n",
    "In the end, it turns out that the prior mean (which is the mean function here) is unimportant for that.\n",
    "Therefore, we can just implement the mean function as constant zero. This is especially true, as we tend to standardize our data so that is is zero mean.\n",
    "\n",
    "**Your task:** Implement the mean function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:03.317176932Z",
     "start_time": "2023-05-08T11:54:03.314652945Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mean(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns the mean. \n",
    "\n",
    "    Note: The mean should be a two-dimensional column vector - one 0 per data point. \n",
    "    \n",
    "    If x is a 1d vector with d elements, reshape it to a (1 x d) matrix first.\n",
    "    Should return a 2d zeros vector.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # ‚û°Ô∏è TODO : reshape to 2d ‚¨ÖÔ∏è\n",
    "    \n",
    "    # ‚û°Ô∏è TODO : implement the mean function ‚¨ÖÔ∏è\n",
    "\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Safety check\n",
    "\n",
    "The output of the following cell should be __exactly__ `array([[0.], [0.], [0.]]) (1, 1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:03.411050475Z",
     "start_time": "2023-05-08T11:54:03.318046991Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(mean(np.zeros((3, 4))), mean(np.zeros(4)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple GP implementation\n",
    "\n",
    "We will now use the mean and covariance/kernel functions to build a GP. \n",
    "In the `__call__` function, we will compute the posterior distribution for a vector of points `x`. The output will __not__ be an array, but a __multivariate Gaussian__ with the dimensionality of the length of `x`.\n",
    "\n",
    "As mentioned above, but here with slightly more detailed notation, the posterior mean $\\mu_n$ of a GP is calculated as follows:\n",
    "$$\n",
    "\\mu_n(\\mathbf{x^*}) = k(\\mathbf{x^*},\\mathbf{x}_{1:n})k(\\mathbf{x}_{1:n},\\mathbf{x}_{1:n})^{-1}(y(\\mathbf{x}_{1:n})-\\mu(\\mathbf{x}_{1:n}))+\\mu(\\mathbf{x^*}).\n",
    "$$\n",
    " $\\mathbf{x}_{1:n}$ is the training data for which we have observed function values and $\\mathbf{x^*}$ is the point for which we want to make predictions.\n",
    "\n",
    "The posterior variance is calculated as \n",
    "$$\n",
    "\\sigma_n(\\mathbf{x}) = k(\\mathbf{x^*},\\mathbf{x^*})-k(\\mathbf{x^*},\\mathbf{x}_{1:n})k(\\mathbf{x}_{1:n},\\mathbf{x}_{1:n})^{-1}k(\\mathbf{x^*},\\mathbf{x}_{1:n})^{\\intercal}.\n",
    "$$\n",
    "\n",
    "In the `fit`  function, we will set the training data and calculate a critical component: the inverse of the covariance matrix $k(\\mathbf{x}_{1:n},\\mathbf{x}_{1:n})^{-1}$.\n",
    "Usually, this is done using Cholesky decomposition but we will use `np.linalg.inv` to calculate the inverse.\n",
    "\n",
    "Pro tip: You might run into problems when computing the inverse due to numerical instabilities. If this happens, add a value of $10^{-6}$ to the diagonal elements of the matrix before computing the inverse.\n",
    "\n",
    "**Your task:** Compute the covariance matrix $k(\\mathbf{x}_{1:n},\\mathbf{x}_{1:n})$ and its inverse in `initialize` and calculate the posterior distribution in `__call__`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:03.426815886Z",
     "start_time": "2023-05-08T11:54:03.363421012Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GaussianProcess:\n",
    "    def __init__(\n",
    "            self,\n",
    "            mean_function: Callable[[np.ndarray], np.ndarray],\n",
    "            kern_function: Callable[[np.ndarray, np.ndarray], np.ndarray],\n",
    "            ls: float\n",
    "    ):\n",
    "        # setting the mean function\n",
    "        self.mean = mean_function\n",
    "        # setting the kernel function, fix the lengthscale so we don't have to pass it all the time\n",
    "        self.kern = lambda x, y: kern_function(x, y, ls)\n",
    "        self.x_train = None\n",
    "        self.y_train = None\n",
    "\n",
    "    def initialize(self, x_train: np.ndarray, y_train: np.ndarray) -> 'GaussianProcess':\n",
    "        # some checks\n",
    "        assert x_train.ndim == 2 and y_train.ndim == 2, 'x_train and y_train should be 2D arrays'\n",
    "        assert x_train.shape[0] == y_train.shape[0], 'first dimension of x_train and y_train has to be equal (n_points)'\n",
    "        assert y_train.shape[1] == 1, 'y_train has to be of form (n_points, 1)'\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        # Due to numerical instabilities, the covariance matrix might not be invertible.\n",
    "        # We add a small constant value to the diagonal elements ('jitter') to enfore the\n",
    "        # matrix to be positive semidefinite (which implies invertibility)\n",
    "        # See, e.g., https://scicomp.stackexchange.com/questions/36342/advantage-of-diagonal-jitter-for-numerical-stability\n",
    "        #\n",
    "        #\n",
    "        # The inputs x_train and y_train both have to be 2D here. x_train has shape (n_points, dim_of_points), y_train has\n",
    "        # shape (n_points, 1)\n",
    "        #\n",
    "        # ‚û°Ô∏è TODO : compute the covariance and the inverse of the covariance matrix here. save them as class attributes so we don't need to\n",
    "        #  recompute them all the time ‚¨ÖÔ∏è\n",
    "        #\n",
    "        self.cov = ...\n",
    "        self.cov_inv = ...\n",
    "        return self\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> sp.stats._multivariate.multivariate_normal_frozen:\n",
    "        assert self.x_train is not None and self.y_train is not None, \"Have to initialize the GP before calling\"\n",
    "        #\n",
    "        # ‚û°Ô∏è TODO : compute the posterior distribution ‚¨ÖÔ∏è\n",
    "        #\n",
    "        # First, compute the posterior mean and covariance (use self.cov_inv and the definitions above), \n",
    "        # then compute the posterior distribution which is a multivariate normal distribution\n",
    "        # Hint: you might need to add a small diagonal value to the covariance matrix again\n",
    "        # Use the code from above for that (don't add more than 1e-6)\n",
    "\n",
    "        if x.ndim == 1:\n",
    "            x = x[np.newaxis, :]\n",
    "            \n",
    "        return ...\n",
    "\n",
    "    def posterior_mean(self, x: np.ndarray) -> np.ndarray:\n",
    "        if x.ndim == 1:\n",
    "            x = x[np.newaxis, :]\n",
    "        dist = self(x)\n",
    "        return dist.mean\n",
    "\n",
    "    def posterior_covariance(self, x: np.ndarray) -> np.ndarray:\n",
    "        if x.ndim == 1:\n",
    "            x = x[np.newaxis, :]\n",
    "        dist = self(x)\n",
    "        return dist.cov\n",
    "\n",
    "    def log_marginal_likelihood(\n",
    "            self,\n",
    "    ) -> float:\n",
    "        #\n",
    "        # ‚û°Ô∏è TODO : Implement this when you're instructed to in the notebook, you can skip this for now otherwise ‚¨ÖÔ∏è\n",
    "        # \n",
    "        \n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Safety check:** Run the following cell to make sure that everything is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:57:02.658731126Z",
     "start_time": "2023-05-08T11:57:02.579452197Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if isinstance(\n",
    "        GaussianProcess(mean, kern, np.random.rand()).initialize(np.random.rand(5, 1), np.random.rand(5, 1))(\n",
    "            np.random.rand(3, 1)\n",
    "        ),\n",
    "        sp.stats._multivariate.multivariate_normal_frozen\n",
    "):\n",
    "    print(\"‚úÖ All good.\")\n",
    "else:\n",
    "    print(\"üö® __call__ does not return a multivariate distribution. Make sure to use 'sp.stats.multivariate_normal'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Initializing and fitting the Gaussian process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:03.451831504Z",
     "start_time": "2023-05-08T11:54:03.363909558Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "# ‚û°Ô∏è TODO : Create a new GP instance with a lengthscale of your choice. \n",
    "# Try different lengthscales to see how the affect the behavior of the GP.  ‚¨ÖÔ∏è\n",
    "#       \n",
    "\n",
    "gp = ...\n",
    "gp.initialize(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:03.452056958Z",
     "start_time": "2023-05-08T11:54:03.364179772Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_gp(gp: GaussianProcess, x_test: np.ndarray, y_test: np.ndarray, ax: plt.Axes = None) -> None:\n",
    "    \"\"\"\n",
    "    Plot the GP posterior distribution of gp for a given set of test points (x_test and y_test).\n",
    "    Accepts an optional parameter ax which plots it on an existing ax, otherwise a new figure\n",
    "    is create.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "    posterior_distribution = gp(x_test)\n",
    "    rvs = posterior_distribution.rvs(10)\n",
    "\n",
    "    ci_lb = []\n",
    "    ci_ub = []\n",
    "\n",
    "    for _x in x_test:\n",
    "        x_marginal = gp(_x)\n",
    "        _mean = x_marginal.mean.squeeze()\n",
    "        variance = x_marginal.cov.squeeze()\n",
    "\n",
    "        _lb, _ub = sp.stats.norm.interval(0.95, loc=_mean, scale=np.sqrt(variance))\n",
    "        ci_lb.append(_lb)\n",
    "        ci_ub.append(_ub)\n",
    "\n",
    "    ci_lb = np.array(ci_lb)\n",
    "    ci_ub = np.array(ci_ub)\n",
    "\n",
    "    for i, rv in enumerate(rvs):\n",
    "        x_test_sq = x_test.squeeze()\n",
    "        ax.plot(x_test.squeeze(), rv, alpha=0.25, color='blue', label='posterior sample' if i == 0 else None)\n",
    "    ax.fill_between(x_test.squeeze(), ci_lb, ci_ub, color='gray', alpha=0.5, label='95% CI')\n",
    "    ax.plot(x_test, y_test, color='red', label='y(x)')\n",
    "    ax.scatter(x_train, y_train, marker='x', color='black', label='training data')\n",
    "    ax.set_ylabel('y(x)')\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the above GP for the test data defined earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:03.941063193Z",
     "start_time": "2023-05-08T11:54:03.406881598Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_gp(gp, x_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot we see the predicted mean, the 95% error bars together with a few functions sampled from the Gaussian distribution.\n",
    "\n",
    "# Task: play around with lengthscales\n",
    "\n",
    "* What happens if too low?\n",
    "* What happens if too high?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Systematic approach to fit lengthscales: MLE\n",
    "\n",
    "We saw above that setting the lengthscale incorrectly can render the Gaussian process useless: if it is set too low or too high, the Gaussian process fails in modelling the function reasonably (if you didn't see that, try lengthscales of 0.1 and 10).\n",
    "While you probably found a lengthscale value that led to reasonable performance, we want to set the lengthscale automatically because what a \"good value\" is depends on the function at hand.\n",
    "All approaches to set the lengthscale (and often more hyperparameters) are in some form **maximizing the marginal likelihood of seeing the training data under the GP prior** (i.e., we will optimize our lengthscale with MLE).\n",
    "\n",
    "\n",
    "Note that $\\mathbf{y}$ is a $n$-dimensional vector which represents one possible realization of the underlying true function at the locations $X \\in \\mathbb{R}^{n\\times d}$.\n",
    "The vector $\\mathbf{y}|X$ follows a multivariate normal distribution, and as such:\n",
    "$$\n",
    "\\hat{l} = \\arg\\max_l p(\\mathbf{y}|X,l)\n",
    "$$\n",
    "\n",
    "We state the posterior (log) marginal likelihood here and refer to [Gaussian Processes for Machine Learning](https://gaussianprocess.org/gpml/chapters/RW.pdf) for a derivation:\n",
    "$$\n",
    "\\log p(\\mathbf{y}|X,l) = -\\frac{1}{2}\\mathbf{y}^\\intercal K^{-1}\\mathbf{y}-\\frac{1}{2}\\log |K| -\\frac{n}{2}\\log 2\\pi.\n",
    "$$\n",
    "\n",
    "Again, we switch between the notation $K$ and $k(x_{1:n}, x_{1:n})$, but they mean the same thing.\n",
    "\n",
    "\n",
    "**YOUR TASK**: Implement the `log_marginal_likelihood` in the `GP` class above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:03.941274568Z",
     "start_time": "2023-05-08T11:54:03.936674720Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We define a function that we can pass to the minimize function from scikit learn\n",
    "\n",
    "def negative_marginal_log_likelihood(log_ls: float) -> float:\n",
    "    ls = np.exp(log_ls)\n",
    "    gp = GaussianProcess(mean, kern, ls)\n",
    "    gp.initialize(x_train, y_train)\n",
    "    return -gp.log_marginal_likelihood()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximize the marginal likelihood by minimizing the negative marginal log likelihood\n",
    "\n",
    "We optimize the negative_marginal_log_likelihood over the log of the lengthscale intead of directly over the parameter.\n",
    "\n",
    "**Question:** What is the benefit of doing that?\n",
    "\n",
    "**Safety check:** The following cell should print something close to `[0.76656]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:04.058849379Z",
     "start_time": "2023-05-08T11:54:03.939494104Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_log_ls = sp.optimize.minimize(negative_marginal_log_likelihood, 1)['x']\n",
    "best_ls = np.exp(best_log_ls)\n",
    "print(best_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the GP with lengthscale fitted by MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:04.627587887Z",
     "start_time": "2023-05-08T11:54:03.989725720Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gp = GaussianProcess(mean, kern, best_ls)\n",
    "gp.initialize(x_train, y_train)\n",
    "\n",
    "posterior_distribution = gp(x_test)\n",
    "\n",
    "plot_gp(gp, x_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Towards Bayesian Optimization: Acquisition functions\n",
    "\n",
    "We now have all the ingredients of the Gaussian process and focus on the actual task: finding the optimizer of a function (i.e., the point with the best function value).\n",
    "The general strategy is to first fit a Gaussian process on a small number of initial training points and then find the next point to evaluate.\n",
    "\n",
    "We find this next point by maximizing a so-called _acquisition function_.\n",
    "While it may seem strange to solve an optimization problem (maximizing the acquisition function) to maximize our black-box function, the acquisition function can be maximized using gradient-based approaches because it has a closed-form expression.\n",
    "\n",
    "We will work with a popular acquisition function: __Expected improvement (EI)__.\n",
    "EI describes by how much, in expectation w.r.t. to our GP posterior after $n$ observations, a given point improves over current best function value observed so far $y^*_n$:\n",
    "$$\n",
    "EI_n(x) = \\mathbb{E}_{y\\sim GP(X,\\mathbf{y})}\\left[[y(x)-y^*_n]^+ \\right]\n",
    "$$\n",
    "Note that $[x]^+:=\\max(0,x)$.\n",
    "\n",
    "EI naturally does an _exploration-exploitation tradeoff_: points that have already been evaluated get zero EI (in noiseless models as in this lab) but regions where all possible functions have a value worse than the current best point also get zero EI. \n",
    "The sweet spot are regions that are under-explored but are also promising.\n",
    "\n",
    "Since our GP posterior follows a multivariate normal distribution, we can find a [closed form expression for the expected improvement](https://ekamperi.github.io/machine%20learning/2021/06/11/acquisition-functions.html). Let the mean and variance of our new point $(x,y)$ be $\\mu(x)$ and $\\sigma(x)$, as given by the GP. Then:\n",
    "$$\n",
    "EI_n(x) = \\sigma(x)\\varphi (Z(x)) + \\sigma(x)Z(x)\\Phi (Z(x)),\n",
    "$$\n",
    "where $Z(x) = \\frac{\\mu_n(x)-y^*_n}{\\sigma}$ is the expected difference between the point $x$ and the best function values after $n$ observations. Further, $\\varphi(x)$ is the probability density function of the *standard* normal distribution, and $\\Phi$ is the cummulative density function of the *standard* normal distribution.\n",
    "\n",
    "__YOUR TASK:__ Implement the expected improvement acquisition function by adding missing lines in the cell below. You can use sp.stats for pds and cdfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:04.632256151Z",
     "start_time": "2023-05-08T11:54:04.631077144Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def expected_improvement(\n",
    "    gp: GaussianProcess,\n",
    "    x: np.ndarray,\n",
    "    best_observed_value: float\n",
    ") -> np.ndarray:\n",
    "    \n",
    "    if x.ndim == 1:\n",
    "        x = x[np.newaxis, :]\n",
    "    eis = []\n",
    "    for _x in x:\n",
    "        # ‚û°Ô∏è TODO : Implement the expected improvement acquisition function by adding missing lines  ‚¨ÖÔ∏è\n",
    "        ...\n",
    "        eis.append(_ei.squeeze())\n",
    "    return np.array(eis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Safety check:__ (make sure you executed all cells in order up to here, i.e., you used the GP with MLE). \n",
    "\n",
    "You should get the following figure:\n",
    "\n",
    "![EI plot](EI.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:05.365253473Z",
     "start_time": "2023-05-08T11:54:04.634117019Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(expected_improvement(gp, np.linspace(lb, ub, 300).reshape(-1, 1), y_train.max()).squeeze(), label=rf'$EI_n(x)$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian optimization\n",
    "\n",
    "Now, we have all the ingredients to do Bayesian optimization. We will \n",
    "* Sample some initial points and evaluate them\n",
    "* Fit a GP model with MLE\n",
    "* Find the next point to evaluate by maximizing the Expected improvement\n",
    "* Evaluate the next point, add it to the data and start over from the second point \n",
    "\n",
    "We will now see how all components play together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a function to maximize the likelihood\n",
    "\n",
    "We wrap our MLE procedure in a single function that we can call conveniently later on. Given `mean`, `kern`, `x_train`, and `y_train`, this function returns a GP where the lengthscale is set by MLE. \n",
    "\n",
    "**Question (after running the BO loop):** Why is the model of the function so accurate in the middle part of the function but poor in the outer parts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:05.365765649Z",
     "start_time": "2023-05-08T11:54:05.360057884Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def maximize_likelihood(\n",
    "        mean: Callable[[np.ndarray], np.ndarray],\n",
    "        kern: Callable[[np.ndarray], np.ndarray],\n",
    "        x_train: np.ndarray, y_train: np.ndarray\n",
    ") -> GaussianProcess:\n",
    "    gp = GaussianProcess(mean, kern, 0.1)\n",
    "    gp.initialize(x_train, y_train)\n",
    "\n",
    "    def likelihood_function(log_ls: float) -> float:\n",
    "        gp = GaussianProcess(mean, kern, np.exp(log_ls))\n",
    "        gp.initialize(x_train, y_train)\n",
    "        return -gp.log_marginal_likelihood()\n",
    "\n",
    "    best_log_ls = sp.optimize.minimize(likelihood_function, 1)['x']\n",
    "    best_ls = np.exp(best_log_ls)\n",
    "    \n",
    "    gp = GaussianProcess(mean, kern, best_ls)\n",
    "    gp.initialize(x_train, y_train)\n",
    "\n",
    "    return gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:19.081771031Z",
     "start_time": "2023-05-08T11:54:05.368826259Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train = np.random.RandomState(2).rand(5) * (ub - lb) + lb\n",
    "x_train = x_train[:, np.newaxis]\n",
    "y_train = fn(x_train)\n",
    "\n",
    "gp = maximize_likelihood(mean, kern, x_train, y_train)\n",
    "ei = lambda x: expected_improvement(gp, x, y_train.max())\n",
    "\n",
    "for i in range(10):\n",
    "    print('###########################')\n",
    "    print(f\"####### iteration {i} #######\")\n",
    "    print('###########################')\n",
    "\n",
    "    fig, axs = plt.subplots(2, 1, sharex=True, figsize=(7, 7))\n",
    "    # plot current GP\n",
    "    plot_gp(gp, x_test, y_test, ax=axs[0])\n",
    "    # stupidly optimize the acquisition function\n",
    "    x_range = np.linspace(lb, ub, 1000)[:, np.newaxis]\n",
    "    eis = ei(x_range)\n",
    "    x_next = x_range[np.argmax(eis)]\n",
    "\n",
    "    # plot acquisition function\n",
    "    axs[1].plot(x_range, eis)\n",
    "    axs[1].set_ylabel('EI(x)')\n",
    "    axs[1].vlines(x_next, ymin=ei(x_next), ymax=0, color='green', linestyle='dashed', label='best x')\n",
    "    axs[1].legend()\n",
    "    plt.show(fig)\n",
    "    # evaluate point where acquisition function maximum\n",
    "    y_next = fn(x_next)\n",
    "    # add to training data\n",
    "    x_train = np.vstack((x_train, x_next[np.newaxis, :]))\n",
    "    y_train = np.vstack((y_train, y_next[np.newaxis, :]))\n",
    "\n",
    "    gp = maximize_likelihood(mean, kern, x_train, y_train)\n",
    "    ei = lambda x: expected_improvement(gp, x, y_train.max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## PART 2\n",
    "***\n",
    "# Bayesian optimization in action\n",
    "\n",
    "In the second part of this lab, we will use Bayesian optimization to tune the hyperparameters of a neural network. Now when we have a little deeper understanding of how the Bayesian Optimization works internally, we will switch over and use a professional implementation, which is what one would use in practice. Here we will use the [BoTorch](https://botorch.org/) library developed at Meta, which is arguably the most efficient and well maintaned library at the moment. It uses the [Gpytorch](https://gpytorch.ai/) gaussian process library developed at Cornell University. \n",
    "\n",
    "We will use the white wine dataset, which contains ~5.000 different wines with physical attributes and a taste score from users that goes from 1 to 10. Our goal is to train a predictor to guess the quality of the wine from its physical attributes. The data comes from the paper\n",
    "\n",
    "P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. \n",
    "Modeling wine preferences by data mining from physicochemical properties.\n",
    "In Decision Support Systems, Elsevier, 47(4):547-553. ISSN: 0167-9236.\n",
    "\n",
    "However, we only use a subset of the data set to speed up training.\n",
    "\n",
    "The predictor will be a small (though probably too large) ANN, and the parameters that we will optimize now are hyperparameters of this ANN. Especially, we will optimize:\n",
    "- Number of layers\n",
    "- Number of neurons per layer\n",
    "- Learning rate\n",
    "- Dropout rate\n",
    "- Number of training iterations\n",
    "\n",
    "NOTE!!\n",
    "The two parts of this lab are completely standalone, and as such, we will not re-use anything from Part 1 in Part 2.\n",
    "\n",
    "![Wine plot](wine.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T14:13:06.340764Z",
     "start_time": "2024-03-19T14:13:06.328410Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Selecting the appropriate training device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset\n",
    "\n",
    "First we load the data set and split it into train and test sets.\n",
    "We select a small subset of the data for now, as it will be very slow to work with otherwise.\n",
    "\n",
    "- Choose a number of data points to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:21.893921914Z",
     "start_time": "2023-05-08T11:54:20.300668225Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"winequality-white.csv\", delimiter=\";\")\n",
    "print(df.head())\n",
    "print(df.describe())\n",
    "\n",
    "# to speed up training we only use a very small subset of the data set. Feel free to play around. Especially if you have GPU support.\n",
    "# ‚û°Ô∏è TODO : select how many data samples to use in the lab. We recommend 50 or 100 for now.  ‚¨ÖÔ∏è\n",
    "n_data_samples = ...\n",
    "\n",
    "X = df.values[:n_data_samples, :-1]\n",
    "y = df.values[:n_data_samples, -1]\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# to keep this data separate from the X, y used in the bayesian optimization below, we use a D in the name \n",
    "DX_train = torch.tensor(X_train).to(dtype=torch.double, device=device)\n",
    "DX_test = torch.tensor(X_test).to(dtype=torch.double, device=device)\n",
    "Dy_train = torch.tensor(y_train).to(dtype=torch.double, device=device)\n",
    "Dy_test = torch.tensor(y_test).to(dtype=torch.double, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a relatively small ANN architecture with up to 4 hidden layers. We use dropout, with dropout probabilities that are subject to optimization. Furthermore, use ReLU activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:21.896721910Z",
     "start_time": "2023-05-08T11:54:21.894445535Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Here we define the general architecture for the ANN. It takes 3 parameter: number of hidden layers, \n",
    "# number of neurons per hidden layer and dropout probability.\n",
    "\n",
    "class ANN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_hidden_layers: int,\n",
    "        hidden_width: int,\n",
    "        dropout_p: float\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = torch.nn.Sequential(            \n",
    "            torch.nn.Linear(torch.tensor(11), hidden_width),\n",
    "            torch.nn.Dropout(dropout_p),\n",
    "            torch.nn.ReLU(),\n",
    "            *(\n",
    "            torch.nn.Linear(hidden_width, hidden_width),                \n",
    "            torch.nn.Dropout(dropout_p),\n",
    "            torch.nn.ReLU(),\n",
    "            ) * (n_hidden_layers),        \n",
    "            torch.nn.Linear(hidden_width, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Adam optimizer with a learning rate that is subject to optimization. We will use the Mean Squared Error loss function.\n",
    "\n",
    "To make the BO cleaner, we define the full training procedure in a function `black_box_function` that takes a vector of hyperparameters as input and returns the accuracy on the test set as output. We will use this function as a black box in the optimization. \n",
    "\n",
    "Note!! The black_box_function takes inputs in the range [0,1] and then maps those to pre-set ranges inside the black_box_function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:23.436243046Z",
     "start_time": "2023-05-08T11:54:23.434045709Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def black_box_function(x: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    This function takes a (nx5) input tensor x, where each value is between 0 and 1 (inclusive).\n",
    "    It then maps those 0-1 values into the actual values that the ANN needs.\n",
    "    \n",
    "    x[0]: number of hidden layers\n",
    "    x[1]: number of neurons per layer\n",
    "    x[2]: dropout p\n",
    "    x[3]: learning rate\n",
    "    x[4]: num training iterations\n",
    "    \"\"\"\n",
    "\n",
    "    # Map the values\n",
    "    n_hidden = int(torch.floor(x[0] * 3)) # maps from [0,1] -> {0,..,3}\n",
    "    n_neurons = int(torch.floor(x[1] * 196)) + 5 # maps from [0,1] -> {5, 200}\n",
    "    dropout_p = 0.1 * x[2] # maps from [0,1] -> [0, 0.1]\n",
    "    learning_rate = 0.1 * x[3] # maps from [0,1] -> [0, 0.1]\n",
    "    num_gd_iters = int(torch.floor(x[4] * 901) + 100) # maps from [0,1] -> {100, 1000}\n",
    "\n",
    "    print(f'dropout: {dropout_p:.3E}', end='\\t')\n",
    "    print(f'lr: {learning_rate:.3E}', end='\\t')\n",
    "    print(f'depth: {n_hidden}', end='\\t')\n",
    "    print(f'width: {n_neurons}', end='\\t')\n",
    "    print(f'gd_iter: {num_gd_iters}')\n",
    "\n",
    "    # create the model\n",
    "    model = ANN(int(n_hidden), int(n_neurons), dropout_p).to(dtype=torch.double, device=device)\n",
    "\n",
    "    # Train the model\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate.item())    \n",
    "    model.train()\n",
    "    for epoch in range(num_gd_iters):\n",
    "        y_pred = model(DX_train)\n",
    "        loss = loss_fn(y_pred.reshape(-1), Dy_train.reshape(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "    model.eval()\n",
    "\n",
    "    # calculate train and test loss\n",
    "    with torch.no_grad():\n",
    "        y_train_pred = model(DX_train)\n",
    "        y_test_pred = model(DX_test)\n",
    "        train_loss = loss_fn(y_train_pred.reshape(-1), Dy_train.reshape(-1))\n",
    "        test_loss = loss_fn(y_test_pred.reshape(-1), Dy_test.reshape(-1))\n",
    "        print(f\"Test loss = {test_loss:.3f}\", f\"[Training loss = {train_loss:.3f}]\")\n",
    "        \n",
    "\n",
    "    # We minimize the test loss\n",
    "    return test_loss.detach().cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We test that the black box function works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:23.436243046Z",
     "start_time": "2023-05-08T11:54:23.434045709Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "default_performance = black_box_function(torch.tensor([0, 0,  0, 0, 0], dtype=torch.double, device=device))\n",
    "print(\"test loss of default hyperparameters:\", default_performance.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the GP model\n",
    "\n",
    "We now have a model to optimize.\n",
    "\n",
    "**Question:** What are we optimizing?\n",
    "\n",
    "**Question:** What variables are we optimizing over?\n",
    "\n",
    "Next thing is to create the BO loop.\n",
    "\n",
    "We model the GP with GPyTorch. GPs can also model noisy functions. We haven't talked about noise above but the definitions of a noisy GP are almost identical to the ones of noiseless GPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:23.617987989Z",
     "start_time": "2023-05-08T11:54:23.438063797Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "\n",
    "from botorch.models import SingleTaskGP\n",
    "\n",
    "\n",
    "def get_gp(x_train: torch.Tensor, y_train: torch.Tensor) -> SingleTaskGP:\n",
    "    # ‚û°Ô∏è TODO : Create a new SingleTaskGP and return the model  ‚¨ÖÔ∏è\n",
    "    # See https://botorch.org/api/_modules/botorch/models/gp_regression.html#SingleTaskGP for hints\n",
    "    # You can try to improve your GP by choosing another kernel, another length scale prior, or \n",
    "    # another prior for the likelihood. The default should be fine though.\n",
    "\n",
    "    model = ...\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cheap function to test if everything works\n",
    "\n",
    "You can use the following test function to check if your BO loop runs as intended before running on the expensive HPO problem. On the Branin problem, you should reach a value of around 0.4 after approximately 200 function evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from botorch.test_functions import Branin\n",
    "\n",
    "cheap_function = Branin()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will random sample d+1 initial points to use as initial training data for the GP before starting to generate new points with BO. \n",
    "As above, we use Expected Improvement as the acquisition function. We will optimize the acquisition function using the L-BFGS algorithm. The Bayesian optimization loop will run for 24 iterations (and might take some time).\n",
    "\n",
    "Note!! In part 1, we tried to maximize a function. In part two we are instead minimizing. Also, be careful with normalization/standardization. We want to train our GP model on normalized/standardardized data, but when we want to run our black box function, that takes unnormalized data as input. For the ANN blackbox it matters less, as that has [0,1] bounds already, but for the cheap test function it is important.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:56:17.210579536Z",
     "start_time": "2023-05-08T11:54:23.614538186Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from botorch.optim import optimize_acqf\n",
    "from botorch.optim import optimize_acqf_mixed\n",
    "from botorch.acquisition import LogExpectedImprovement\n",
    "from gpytorch import ExactMarginalLogLikelihood\n",
    "from botorch.utils.transforms import normalize, unnormalize, standardize\n",
    "from botorch.fit import fit_gpytorch_mll\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "# ‚û°Ô∏è TODO : Set this to 'cheap_function' or 'black_box_function' \n",
    "function_to_optimize = ...\n",
    "\n",
    "dim = 2 if isinstance(function_to_optimize, Branin) else 5\n",
    "\n",
    "if not isinstance(function_to_optimize, Branin):\n",
    "    \"\"\"\n",
    "    Some of are parameters in the HPO task are discrete. And while the black box function is written as to accept continuous inputs,\n",
    "    the performance will be better if we let the BO algorithm know that it should only try discrete values.\n",
    "\n",
    "    We do this by replacing the normal acquisition function optimizer \"optimize_acqf()\" from the botorch library with \"optimize_acqf_mixed\",\n",
    "    where mixed stands for mixed continuous and discrete inputs.\n",
    "\n",
    "    This mixed acqf optimizer requries a list of all points it should try for the discrete parameters which we calculate\n",
    "    below and call fixed_feature_list. It is just the cartesian product of all discrete values. Note that we still\n",
    "    normalize everything to [0, 1].\n",
    "    \"\"\"\n",
    "    options_n_hidden = np.arange(4) / 3\n",
    "    options_n_neurons = (np.arange(5, 201) - 5) / 201\n",
    "    options_gd_iters = (np.arange(100, 1001) - 100) / 901\n",
    "\n",
    "    fixed_features_list = []\n",
    "    # iterate over the cartesian product of feasible locations and add a dictionary entry for each\n",
    "    for _n_hid, _n_neur, _n_gd_it in itertools.product(options_n_hidden, options_n_neurons, options_gd_iters):\n",
    "        fixed_features_list.append({\n",
    "            0: _n_hid,  # 0: index of 'n_hidden'\n",
    "            1: _n_neur,  # 1: index of 'n_neurons'\n",
    "            4: _n_gd_it  # 2: # 0: index of 'n_gd_iters'\n",
    "        })\n",
    "    n_fixed_features = len(fixed_features_list)  # this is a rather long list\n",
    "\n",
    "\n",
    "# First we need some random samples to train our GP on\n",
    "# We use dim+1 initial random samples\n",
    "x_init = torch.rand(dim+1, dim, dtype=torch.float64)\n",
    "\n",
    "# Next we need to evaluate the black box function on each of those values\n",
    "y_init = torch.tensor([function_to_optimize(x) for x in x_init]).reshape(-1, 1)\n",
    "\n",
    "x = x_init\n",
    "y = y_init\n",
    "\n",
    "# Set to 50 for cheap function and to 24 for the HPO problem.\n",
    "N_BO_STEPS = 50 if isinstance(function_to_optimize, Branin) else 24\n",
    "# Set to 1 for the HPO problem\n",
    "PRINT_EVERY = 5 if isinstance(function_to_optimize, Branin) else 1\n",
    "\n",
    "print('*** Starting Bayesian Optimization ***')\n",
    "for gp_iter in range(N_BO_STEPS):\n",
    "    if gp_iter % PRINT_EVERY == 0:\n",
    "        print(f'** Iteration {gp_iter + 1}/{N_BO_STEPS} **')\n",
    "    # We define the bounds for the optimization. We assume that all hyperparameters are between 0 and 1.\n",
    "    bounds = torch.stack([torch.zeros(dim), torch.ones(dim)]) if not isinstance(function_to_optimize, Branin) else torch.stack([torch.tensor([-5, 0]), torch.tensor([10, 15])])\n",
    "\n",
    "    # ‚û°Ô∏è TODO : Normalize the x values to be between 0 and 1. You may use the botorch transforms  \n",
    "    # https://botorch.org/api/_modules/botorch/utils/transforms.html    \n",
    "    x_norm = ...\n",
    "\n",
    "    # ‚û°Ô∏è TODO : Standardize the y values to have mean zero and standard deviation one.  \n",
    "    y_stand = ...\n",
    "\n",
    "    # ‚û°Ô∏è TODO : Create a new GP with the normalized x and y values \n",
    "    gp = ...\n",
    "\n",
    "    # Your GP model has an attribute `likelihood` which you can use to compute the marginal log likelihood.\n",
    "    # This attribute gives the term p(y|f,X,l) in the marginal log likelihood which in our case\n",
    "    # is a Gaussian (see https://docs.gpytorch.ai/en/stable/likelihoods.html#gaussianlikelihood )\n",
    "    # ‚û°Ô∏è TODO : Define the marginal log likelihood of the model (see https://docs.gpytorch.ai/en/stable/marginal_log_likelihoods.html#exactmarginalloglikelihood ) \n",
    "    # ‚û°Ô∏è TODO : Train the model (see https://botorch.org/api/fit.html) \n",
    "    mll = ...\n",
    "\n",
    "    # ‚û°Ô∏è TODO : fit the model by maximizing the marginal likelihood (see https://botorch.org/api/fit.html#botorch.fit.fit_gpytorch_mll)\n",
    "    ...\n",
    "\n",
    "    # ‚û°Ô∏è TODO : Define an acquisition function for your model. We'll use the Log Expected Improvement (see https://botorch.org/api/_modules/botorch/acquisition/analytic.html#ExpectedImprovement )  \n",
    "    # Note that we are minimizing now, so pass `maximize=False` as an argument.\n",
    "    ei = ...\n",
    "\n",
    "    # optimizing the acquisition function\n",
    "    # set up the optimizer (for HPO and toy problem respectively)\n",
    "    if not isinstance(function_to_optimize, Branin):\n",
    "        # For the HPO task, we need to pass the fixed_features_list to the optimizer\n",
    "        # however, we shuffle our list of values for discrete parameters and only keep the first 100 to keep a reasonably fast optimization\n",
    "        optimizer = optimize_acqf_mixed\n",
    "        random.shuffle(fixed_features_list)\n",
    "        fixed_features_list_subsample = fixed_features_list[:100]\n",
    "        kwargs = {\n",
    "            \"fixed_features_list\": fixed_features_list_subsample\n",
    "        }\n",
    "    else:\n",
    "        optimizer = optimize_acqf\n",
    "        kwargs = {}\n",
    "\n",
    "    # call the optimizer\n",
    "    x_next, acq_value = optimizer(\n",
    "        ei,\n",
    "        bounds=torch.stack([torch.zeros(dim), torch.ones(dim)]),\n",
    "        q=1,\n",
    "        num_restarts=3,\n",
    "        raw_samples=1024,        \n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    # We unnormalize x_next to be in the original bounds\n",
    "    x_next_unnorm = unnormalize(x_next, bounds).detach().cpu()\n",
    "\n",
    "    # Lastly we evaluate your black-box function on the point suggested by the acquisition function\n",
    "    y_next = function_to_optimize(x_next_unnorm.squeeze())\n",
    "\n",
    "    if gp_iter % PRINT_EVERY == 0:\n",
    "        print(f'New function value: {y_next.item():.4f}. Current best: {y.min().item():.4f}')\n",
    "        print('\\n')\n",
    "    x = torch.cat([x, x_next_unnorm])\n",
    "    y = torch.cat([y, y_next.reshape(-1, 1)])\n",
    "\n",
    "print(\"Finished!\")\n",
    "\n",
    "x_bo = x\n",
    "y_bo = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing BO to random search\n",
    "\n",
    "We compare the performance of the models found by BO to randomly searching for ANN hyperparameters. Normally, we see that BO outperforms random search. Sometimes more, sometimes less. How does it perform in this case?\n",
    "\n",
    "__Your task:__ Randomly initialize 30 ANNs and evaluate their performance. The performances should be in the 1D tensor `y_rs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-08T11:56:16.646792551Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ‚û°Ô∏è TODO : Randomly initialize 30 ANNs and evaluate their performance.  ‚¨ÖÔ∏è\n",
    "\n",
    "x_rs = ...\n",
    "y_rs = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the evolution of performance\n",
    "\n",
    "Assuming that we have sequential setting where we run one configuration at a time, we can plot the accuracy of best configuration seen so far against time. This way, the y value for the two methods state the performacen we would have stopped after x iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-08T11:56:16.652955668Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_bo_min = np.minimum.accumulate(y_bo)\n",
    "y_rs_min = np.minimum.accumulate(y_rs)\n",
    "\n",
    "plt.plot(y_bo_min, label='Bayesian Optimization')\n",
    "plt.plot(y_rs_min, label='Random Search')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if you want to, you can reuse this BO loop for your other machine learning projects, just replace the black box function for whatever you want to optimize. :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The last thing we want to do is reconnect to the application\n",
    "\n",
    "Find the best x values for the BO and the RNN applications, respectively. Then run the `black_box_function` on both and look at the ANN hyperparameter values it prints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚û°Ô∏è TODO : Randomly initialize 30 ANNs and evaluate their performance.  ‚¨ÖÔ∏è\n",
    "x_best_rs = ...\n",
    "x_best_bo = ...\n",
    "\n",
    "print(\"Best hyperparameter values found for random search\")\n",
    "black_box_function(x_best_rs)\n",
    "print(\"Best hyperparameter values found for random search\")\n",
    "black_box_function(x_best_rs)\n",
    "print(\"Best hyperparameter values found for BO\")\n",
    "black_box_function(x_best_bo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GP model has internal parameters that regulates the parameter importance (lower means more important). You can print them as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hyperparameter, lengthscale in zip(\n",
    "    [\n",
    "        \"number of hidden layers:\",\n",
    "        \"number of neurons per layer:\",\n",
    "        \"dropout:\\t\\t\",\n",
    "        \"learning rate:\\t\\t\",\n",
    "        \"num training iterations:\",\n",
    "    ],\n",
    "    gp.covar_module.lengthscale.squeeze()\n",
    "):\n",
    "    print(hyperparameter, \"\\t\", lengthscale)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Which parameters seem to matter a lot?\n",
    "\n",
    "**Question:** Which parameters seem to matter a little?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
